[View code on GitHub](https://github.com/misbahsy/the-algorithm/twml/libtwml/src/ops/percentile_discretizer_v2.cpp)

The `PercentileDiscretizerV2` operation is used to discretize a tensor containing continuous features, which is useful for the representation of sparse vectors where there are many zeros. The operation maps observation vectors to higher dimensional observation vectors. The input to the operation includes `input_ids`, a tensor containing input feature ids (direct from data record), `input_vals`, a tensor containing input values at corresponding feature ids, `bin_ids`, a tensor containing the discretized feature id for each bin, `bin_vals`, a tensor containing the bin boundaries for values of a given feature, `feature_offsets`, which specifies the starting location of bins for a given feature id, `start_compute`, which specifies which index to start the computation at, `end_compute`, which specifies which index to end the computation right before, `output_bits`, the maximum number of bits to use for the output IDs, `feature_ids`, a 1D TensorProto of feature IDs seen during calibration, `feature_indices`, a 1D TensorProto of feature indices corresponding with feature_IDs, and `cost_per_unit`, an estimate of the number of CPU cycles (or nanoseconds if not CPU-bound) to complete a unit of work. 

The output of the operation includes `new_keys`, the discretized feature ids with the same shape and size as keys, and `new_vals`, the discretized values with the same shape and size as vals. The operation performs the following operation: `(F,x) -> (map(x|F),1)`. Here, `F` is the ID of the feature, and `x` is some real value (i.e., continuous feature). `map(x|F)` is a new feature ID, and the value observed for that feature is 1. For each feature `F`, a (discrete, finite) set of new feature IDs, `newIDs(F)`, is associated. `F~(x)` is in the set `newIDs(F)` for any value of `x`. Each set member of `newIDs(F)` is associated with a 'bin', as defined by the bin boundaries given in the `bin_vals` input array. For any two different feature IDs `F` and `G`, `INTERSECT(newIDs(F),newIDs(G))` is the empty set. 

The `PercentileDiscretizerV2` operation is implemented in C++ using the TensorFlow library. The `PercentileDiscretizerV2` class is a subclass of `OpKernel` and has a template parameter `T` that specifies the data type of the input and output tensors. The constructor of the class extracts the number of output bits and constructs the `ID_to_index` hash map. The `Compute` method of the class calls the `CombinedComputeDiscretizers` function, which performs the actual computation. The `CombinedComputeDiscretizers` function takes the input tensors and the `ID_to_index` hash map as input and outputs the `new_keys` and `new_vals` tensors. The function uses the `twml` library to perform the discretization and the `tensorflow_utils.h` header file to convert between TensorFlow tensors and `twml` tensors. The function also uses the `work_sharder.h` header file to split up the work among multiple threads.
## Questions: 
 1. What is the purpose of this code and what inputs/outputs does it take/produce?
- This code is an implementation of a percentile discretizer algorithm for continuous features in a tensor. It takes in several input tensors including feature ids, feature values, bin ids, bin values, and feature offsets, and produces two output tensors representing the discretized feature ids and values.

2. What is the data structure used to store the mapping between feature IDs and feature indices?
- The data structure used is a hash map, specifically a dense hash map if the macro USE_DENSE_HASH is defined. The hash map is populated with feature IDs as keys and feature indices as values.

3. How is the computation parallelized and what is the estimated cost per unit of work?
- The computation is parallelized using Tensorflow's work_sharder utility, which splits up the work among multiple threads. The estimated cost per unit of work is an input parameter to the algorithm and is used to determine the optimal number of shards to use for parallelization.